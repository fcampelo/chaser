% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/chase_sample_size.R
\name{chase_sample_size}
\alias{chase_sample_size}
\title{Power and sample size calculations for the CHASE procedure}
\usage{
chase_sample_size(N = NULL, cpower = NULL, d = NULL, delta = NULL,
  sigma = NULL, algorithms, alpha, mht.correction = "holm",
  comparetype = c("one-vs-all", "all-vs-all"), direction = c("one-sided",
  "two-sided"))
}
\arguments{
\item{N}{number of instances to be used in the experiment.}

\item{cpower}{desired power for the comparison.}

\item{d}{minimally interesting effect size - standardized (see \code{Usage}
for details).}

\item{delta}{minimally interesting effect size - absolute value (see
\code{Usage} for details).}

\item{sigma}{expected residual standard deviation (see \code{Usage} for
details).}

\item{algorithms}{either an integer representing the number of algorithms to
be compared, or a list object containing the definitions of the
algorithms to be compared (as defined in \code{\link{run_chase}})}

\item{alpha}{significance level (familywise)}

\item{mht.correction}{method used for correcting the value of \code{alpha}
for multiple hypotheses testing (see \code{MHT correction} for
details).}

\item{comparetype}{type of comparison planned (see \code{Types of
comparisons} for details).}

\item{direction}{type of alternative hypotheses planned (see
         \code{Directionality of the alternative hypothesis} for details).}
}
\value{
List object containing the input fields, plus the calculated values
         as described in section \code{Usage}.
}
\description{
Compute the number of instances required for a given comparison of
algorithms, or determine the power curve of the comparision.
}
\details{
This routine calculates the number of instances necessary for a comparison
of \code{K} algorithms, considering a desired power level and a given value
for the minimally interesting effect size (either standardized in terms of
Cohen's d coefficient or given as an absolute value, in which case an
estimate for the residual standard deviation must be provided).

If the number of instaces is predefined (e.g., when using standard benchmark
sets), this routine can return the power for a given effect size, or the
effect size that can be detected with a certain power.
}
\section{Usage}{

This routine can be used in several ways. Five parameters are involved in
power calculations for statistical comparisons of algorithms:

\itemize{
 \item the significance level (\code{alpha})
 \item the number of instances (\code{N})
 \item the power of the comparison (\code{cpower})
 \item the magnitude of the actual difference (\code{delta})
 \item the residual standard deviation (\code{sigma})
}

The two last items in this list can often be combined into a standardized
effect size \code{d = delta/sigma}, that is, in a non-dimensional effect size
indicator that is given as the number of standard deviations.

Depending on which inputs are provided to the routine, it performs different
calculations. The most common usages are listed below:

\strong{Case 1:} \code{N = NULL}, with given \code{d}: calculates the
required number of instances, \code{N}

\strong{Case 2:} \code{N = NULL}, with given \code{delta} and \code{sigma}:
since \code{d = delta / sigma}, this is equivalent to \strong{Case 1})

\strong{Case 3:} \code{d = delta = sigma = NULL}: calculates the smallest
standardized effect size \code{d} that can be detected with power
\code{cpower}.

\strong{Case 4:} \code{cpower = NULL}: calculates the power of the comparison
to detect a standardized effect size of \code{d} or greater.

\strong{Case 5:} \code{d = cpower = NULL}: calculates the power curves
\code{d X cpower} of the comparison.

\strong{Case 6:} \code{N = cpower = NULL}: calculates the power curves
\code{N X cpower} of the comparison.

\strong{Case 7:} \code{N = d = NULL}: calculates the power curves
\code{N X d} of the comparison.
}

\section{MHT correction}{

The power formulas used in this routine are based on the pairwise tests
(e.g., paired t-tests), which mean that in cases where more than 2 algorithms
are to be compared (that is, if there will be \emph{multiple hypotheses
testing} - MHT) the significance level of each test must be corrected to
avoid inflating the familywise error rate.

In the current version only the single-step Holm correction is considered in
the calculation of the sample size. However, if the analysis is eventually
performed using the Bonferroni correction the loss of power is in most cases
quite small.
}

\section{Types of comparisons}{

Depending on the type of comparison being planned, a different set of tests
can be performed: for instance, for a general comparison of methods (in which
there is no algorithm that is of particular interest to the researcher) an
"all-vs-all" comparison (\code{comparetype = "all-vs-all"}) should be
performed, to obtain a complete ordering of performance among the
alternatives. In this case \code{K * (K - 1) / 2} comparisons are performed.
On the other hand, if there is a single method in which the researcher is
particularly interested in (usually the "proposed algorithm") an "all-vs-one"
comparison can be used (\code{comparetype = "one-vs-all"}), since in this
case the question of interest is how one particular algorithm compares
against the others. In this case the total number of comparisons is
\code{K - 1}, which means that the MHT correction of \code{alpha} is smaller,
resulting in a smaller sample size required (or a higher statistical power
for a predefined sample size).
}

\section{Directionality of the alternative hypothesis}{

In the case of an "all-vs-one" comparison, the researcher may be interested
in two kinds of alternative hypotheses: \code{direction = "two-sided"},
if he wants to infer whether the "proposed algorithm" is \emph{different}
from each of the competing approaches; or \code{direction = "one-sided"}, if
he is only interested in determining whether it is \emph{better} than the
others. In the former, the statistical question is
"\emph{is the method better or worse than the others, or is it not
different?}"; in the later, "\emph{is the method better han the others, or is
it not?}". Using a one-sided alternative results in a smaller sample size
required (or a higher statistical power for a predefined sample size).
}

\section{References}{

S. Holm.
"A simple sequentially rejective multiple test procedure".
Scandinavian Journal of Statistics (6):65-70, 1979.

P. Mathews.
"Sample size calculations: Practical methods for engineers and scientists".
Mathews Malnar and Bailey, 2010.

Y. Benjamini, Y.,Hochberg.
"Controlling the false discovery rate: a practical and powerful approach to
multiple testing".
Journal of the Royal Statistical Society Series B 57:289-300, 1995.

Y. Benjamini and D. Yekutieli
"The control of the false discovery rate in multiple testing under
dependency".
Annals of Statistics 29:1165-1188, 2001.
}
\author{
Felipe Campelo (\email{fcampelo@ufmg.br}),
         Fernanda Takahashi (\email{fernandact@ufmg.br})
}

